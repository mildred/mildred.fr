<?xml version="1.0" encoding="utf-8" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="html">Index</title>
  <subtitle type="html"></subtitle>
  <author>
    <name>Administrator</name>
    <uri></uri>
  </author>

  <link href="http://mildred.fr/tags/en/" rel="alternate" />
  <link href="http://mildred.fr/tags/en/atom.xml" rel="self" />
  <generator uri="http://webgen.rubyforge.org/documentation/sourcehandler/feed.html" version="0.5.14">
    webgen - Webgen::SourceHandler::Feed
  </generator>
  <updated>2013-02-14T18:18:38+01:00</updated>
  <id>http://mildred.fr/tags/en/</id>

  
  <entry>
    <title type="html">wwwgen: imagined architecture</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/09/13/wwwgen_imagined_architecture/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/09/13/wwwgen_imagined_architecture/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-09-13T15:30:42+02:00</published>
    
    <content type="html">&lt;p&gt;I started writing wwwgen: a website generator that uses redo for its
dependency tracking. Unfortunately, I might not have taken the correct
approach to the problem. I reuse the webgen concepts, and that might be
a bad idea.&lt;/p&gt;

&lt;p&gt;Specifically, webgen (and my first version of wwwgen) are bottom-up
systems (you take the sources and build everything they can generate).
The problem is that redo itself is top-down (you take the target and
build it, building sources as they are needed), and I tried to make the
two match. It's very difficult, and impossible to do with a clean design.&lt;/p&gt;

&lt;p&gt;What I'd like to have is a simple &lt;code&gt;wwwgen&lt;/code&gt; binary that generates a HTML
file from a source file. Let's imagine how it could work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the source file is a simple page with no templates, just generate
the HTML and this is it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the source file is a simple page with a template, redo-ifchange the
template source and build the HTML combining the two&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the source file is an index, we have a problem because multiple
outputs are generated. Redo doesn't support this case and we must find a
way to make it work.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So, we have a problem here ...&lt;/p&gt;

&lt;p&gt;Now, we have another problem. Specifically, my source file is called
&lt;code&gt;title.page&lt;/code&gt; and I want my output file to be &lt;code&gt;title/index.html&lt;/code&gt;. In
webgen, this is implemented by a configuration in &lt;code&gt;title.page&lt;/code&gt; telling
it to build in &lt;code&gt;title/index.html&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There is a solution to solve both problems at once. the wwwgen command
creates an archive (the formats needs to be defined, it could be tar, or
different yaml documents in the same file for example). Then, the build
process would be:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;all.do&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; find src -name &quot;*.src&quot; \
   | sed 's/src$/gen/' \
   | xargs -d '\n' redo-ifchange
 find src -name &quot;*.gen&quot; \
   | xargs -d '\n' wwwgen unpack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;default.gen.do&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; redo-ifchange &quot;$2.src&quot;
 wwwgen --redo-dependencies -o &quot;$3&quot; generate &quot;$2.src&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;wwwgen generate&lt;/code&gt; would parse the source file and generate an archive,
that will be unpacked later by &lt;code&gt;wwwgen unpack&lt;/code&gt;. Let's see how it can work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The source file can choose where it unpacks, relatively to the
directory where the source file is&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the source file is an index, it will redo-ifchange the other source
files for the index and redo-ifchange the template, generate multiple
pages packed together.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the source file is a tag tree (a special source that doesn't output
anything on its own but create index files dynamically), then it parses
every child to find a canonical list of tags and the paths they refer
to. Then, it creates the index files. Unfortunately, those index files
will never be compiled until next build.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;How an we improve the design to be able to create source files dynamically.&lt;/p&gt;

&lt;p&gt;There are different views to the problem:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;pages, index and tags should all generate all the output files they
are related to. It means that index files should be able to generate
pages, and tags should be able to generate indexes and pages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pages should generate the output file, index should generate pages and
feeds and tags should generate index.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;mixed solution (the one described): pages generate output file, index
should generate the output files as well and tags generates index.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;How can we generate source files on the fly:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;have a predefined compilation order: first tags, then index and lastly
feeds and pages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;rebuild everything until no more source files are generated. We might
build unnecessary things.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I prefer the second solution which is more flexible, but we need a way
to avoid building things twice. For example, it's not necessary to build
a page if on the next phase the page source is going to be regenerated.&lt;/p&gt;

&lt;p&gt;Very simply, the generated page can contain a link to the index source
file that generated it, and when generating the page, &lt;code&gt;redo-ifchange&lt;/code&gt; is
run on the index file.&lt;/p&gt;

&lt;p&gt;Next question: what if a tag is deleted. The corresponding index page is
going to stay around until the next clean. The tag page should keep
around a list of index files it generated and delete them when a tag is
no longer detected. And deleting the index should not be done using &lt;code&gt;rm&lt;/code&gt;
because the index will need to delete the pages it generated. The best
solution would be to integrate to redo to detect these files.&lt;/p&gt;

&lt;p&gt;The build scripts now are:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;all.do&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; oldsrclist=
 srclist=&quot;$(find src -name &quot;*.src&quot;)&quot;
 while [ &quot;$oldsrclist&quot; != &quot;$srclist&quot; ]; do
   echo &quot;$srclist&quot; \
     | sed 's/src$/gen/' \
     | xargs -d '\n' redo-ifchange
   oldsrclist=&quot;$srclist&quot;
   srclist=&quot;$(find src -name &quot;*.src&quot;)&quot;
 done

 find src -name &quot;*.gen&quot; \
   | xargs -d '\n' wwwgen unpack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;default.gen.do&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; redo-ifchange &quot;$2.src&quot;
 wwwgen --redo-dependencies -o &quot;$3&quot; generate &quot;$2.src&quot;
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Git, Google Summer of Code enhancements</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/08/02/git_google_summer_of_code_enhancements/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/08/02/git_google_summer_of_code_enhancements/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-08-02T11:15:20+02:00</published>
    
    <content type="html">&lt;p&gt;I was looking at Git, and which features may land in the next few
releases, and I found the following things:&lt;/p&gt;

&lt;h2&gt;Git-SVN will be completely redesigned&lt;/h2&gt;

&lt;p&gt;If you worked with git-svn, you probably know that the git-svn workflow
has nothing to do with git. Basically, you just have the svn history and
have to use git-svn to push the changes back to the subversion
repository. You can't use git-push and that's really annoying.&lt;/p&gt;

&lt;p&gt;Recently, the &lt;a href=&quot;https://www.kernel.org/pub/software/scm/git/docs/git-remote-helpers.html&quot;&gt;git-remote-helpers&lt;/a&gt; feature was
added. It allows git to interact with any kind of remote url, using a
specific &lt;code&gt;git-remote-*&lt;/code&gt; command. For example, you can already use
mercurial this way (according to &lt;a href=&quot;https://github.com/rfk/git-remote-hg&quot;&gt;git-remote-hg&lt;/a&gt;):&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Git allows pluggable remote repository protocols via helper scripts. If you have a script named &quot;git-remote-XXX&quot; then git will use it to interact with remote repositories whose URLs are of the form XXX::some-url-here. So you can imagine what a script named git-remote-hg will do.&lt;/p&gt;

&lt;p&gt;Yes, this script provides a remote repository implementation that communicates with mercurial. Install it and you can do:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone hg::https://hg.example.com/some-mercurial-repo
$ cd some-mercurial-repo
$ # hackety hackety hack
$ git commit -a
$ git push
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;

&lt;p&gt;The plan is to do the same with subversion. You could just do:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ git clone svn::http://svn.host.org/repo/trunk/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Branches might be tricky to implement, they might not be there. But you
will get what's already in git-svn but with a way better UI. And far
more possibilities for the future.&lt;/p&gt;

&lt;p&gt;Here is the summary from the &lt;a href=&quot;https://git.wiki.kernel.org/index.php/SoC2011Projects#Remote_helper_for_Subversion_and_git-svn&quot;&gt;GSoc&lt;/a&gt; page:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The submodule system of git is very powerful, yet not that easy to work with. This proposed work will strengthen the submodule system even more and improve the user experience when working with submodules.&lt;/p&gt;

&lt;p&gt;Git repository: https://github.com/iveqy/git&lt;/p&gt;

&lt;p&gt;Midterm evaluation: passed&lt;/p&gt;

&lt;p&gt;Progress report / status:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[GSoC 11] submodule improvements at git mailing list&lt;/li&gt;
&lt;li&gt;[GSoC 11 submodule] Status update at git mailing list&lt;/li&gt;
&lt;li&gt;[RFC PATCH] Move git-dir for submodules at git mailing list&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Submodules will be improved a lot&lt;/h2&gt;

&lt;p&gt;I wish it was already there. From the &lt;a href=&quot;https://github.com/jlehmann/git-submod-enhancements/wiki/&quot;&gt;wiki page&lt;/a&gt;, the
improvements will be:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As Dscho put it, submodules are the ‚Äúneglected ugly duckling‚Äù of git. Time to change that ‚Ä¶&lt;/p&gt;

&lt;p&gt;Issues still to be tackled in this repo:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Let am, bisect, checkout, checkout-index, cherry-pick, merge, pull, read-tree, rebase, reset &amp;amp; stash work recursively on submodules (in progress)&lt;/li&gt;
&lt;li&gt;Teach grep the --recursive option&lt;/li&gt;
&lt;li&gt;Add means to specify which submodules shall be populated on clone&lt;/li&gt;
&lt;li&gt;Showing that a submodule has a HEAD not on any branch in ‚Äúgit status‚Äù&lt;/li&gt;
&lt;li&gt;gitk: Add popup menu for submodules to see the detailed history of changes&lt;/li&gt;
&lt;li&gt;Teach ‚Äúgit prune‚Äù the ‚Äú--recurse-submodules‚Äù option (and maybe honour the same default and options ‚Äúgit fetch‚Äù uses)&lt;/li&gt;
&lt;li&gt;Better support for displaying merge conflicts of submodules&lt;/li&gt;
&lt;li&gt;git gui: Add submodule menu for adding and fetching submodules&lt;/li&gt;
&lt;li&gt;git status should call ‚Äúgit diff --submodule --ignore-submodules=dirty‚Äù instead of ‚Äúgit submodule summary‚Äù for providing a submodule summary when configured to do so.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Add an ‚Äúalways-tip‚Äù mode&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Other commands that could benefit from a ‚Äú--recurse-submodules‚Äù option: archive, branch, clean, commit, revert, tag.&lt;/li&gt;
&lt;li&gt;In the long run git-submodule.sh should be converted to a rather simple wrapper script around core git functionality as more and more of that is implemented in the git core.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Submodule related bugs to fix&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cherry picking across submodule creation fails even if the cherry pick doesn‚Äôt touch any file in the submodules path&lt;/li&gt;
&lt;li&gt;git submodule add doesn‚Äôt record the url in .git/config when the submodule path doesn‚Äôt exist.&lt;/li&gt;
&lt;li&gt;git rebase --continue won‚Äôt work if the commit only contains submodule changes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Issues already solved and merged into Junio‚Äôs Repo:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Since git 1.6.6:

&lt;pre&gt;&lt;code&gt;New --submodule option to ‚Äúgit diff‚Äù (many thanks to Dscho for writing the core part!)
Display of submodule summaries instead of plain hashes in git gui and gitk
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.0:

&lt;pre&gt;&lt;code&gt;‚Äúgit status‚Äù and ‚Äúgit diff*‚Äù show submodules with untracked or modified files in their work tree as ‚Äúdirty‚Äù
git gui: New popup menu for submodule diffs
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.1:

&lt;pre&gt;&lt;code&gt;Show the reason why working directories of submodules are dirty (untracked content and/or modified content) in superproject
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.2:

&lt;pre&gt;&lt;code&gt;Add parameters to the ‚Äú--ignore-submodules‚Äù option for ‚Äúgit diff‚Äù and ‚Äúgit status‚Äù to control when a submodule is considered dirty
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.3:

&lt;pre&gt;&lt;code&gt;Add the ‚Äúignore‚Äù config option for the default behaviour of ‚Äúgit diff‚Äù and ‚Äúgit status‚Äù. Both .git/config and .gitmodules are parsed for this option, the value set in .git/config. will override that from .gitmodules
Add a global config option to control when a submodule is considered dirty (written by Dscho)
Better support for merging of submodules (thanks to Heiko Voigt for writing that)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.4:

&lt;pre&gt;&lt;code&gt;Recursive fetching of submodules can be enabled via command line option or configuration.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.5:

&lt;pre&gt;&lt;code&gt;fetch runs recursively on submodules by default when new commits have been recorded for them in the superproject
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.7:

&lt;pre&gt;&lt;code&gt;git push learned the --recurse-submodules=check option which errors out when trying to push a superproject commit where the submodule changes are not pushed (part of Frederik Gustafsson‚Äôs 2011 GSoC project)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Since git 1.7.8:

&lt;pre&gt;&lt;code&gt;The ‚Äúupdate‚Äù option learned the value ‚Äúnone‚Äù which disables ‚Äúsubmodule init‚Äù and ‚Äúsubmodule update‚Äù
The git directory of a newly cloned submodule is stored in the .git directory of the superproject, the submodules work tree contains only a gitfile. This is the first step towards recursive checkout, as it enables us to remove a submodule directory (part of Frederik Gustafsson‚Äôs 2011 GSoC project)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;And the &lt;a href=&quot;https://git.wiki.kernel.org/index.php/SoC2011Projects#Git_submodule_improvements&quot;&gt;GSoC page&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The submodule system of git is very powerful, yet not that easy to work with. This proposed work will strengthen the submodule system even more and improve the user experience when working with submodules.&lt;/p&gt;

&lt;p&gt;Git repository: https://github.com/iveqy/git&lt;/p&gt;

&lt;p&gt;Midterm evaluation: passed&lt;/p&gt;

&lt;p&gt;Progress report / status:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[GSoC 11] submodule improvements at git mailing list&lt;/li&gt;
&lt;li&gt;[GSoC 11 submodule] Status update at git mailing list&lt;/li&gt;
&lt;li&gt;[RFC PATCH] Move git-dir for submodules at git mailing list&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">A New Architecture for the Web</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/07/26/a_new_architecture_for_the_web/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/07/26/a_new_architecture_for_the_web/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-07-26T12:37:15+02:00</published>
    
    <content type="html">&lt;p&gt;I want a new architecture for the web. But what web ?&lt;/p&gt;

&lt;h2&gt;The Traditional Web&lt;/h2&gt;

&lt;p&gt;A Web Site is a collection of Web Pages. A Web Page is a document, a
static document I might add. The document contains text, images, sound
and videos to present some information to the public. This is the
traditional version of the web that I respect very much.&lt;/p&gt;

&lt;p&gt;There is no need to change this as it works very well. The web was
designed to allow a collection of documents to be accessed and does the
job very well&lt;/p&gt;

&lt;p&gt;The one this that is sensible to do however is not to use any scripting
language to render the content of the pages. By definitions, these pages
are static, using a scripting language like PHP is not only going to be
inefficient, but is going to be a security risk.&lt;/p&gt;

&lt;h2&gt;Web Applications&lt;/h2&gt;

&lt;p&gt;This is an entire different thing, and the way we do this is completely
wrong. We think that a web application is a collection of dynamic web
pages. This should be erased from your mind.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Web Application is an application of its own right.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Most unfortunately, applications are written in HTML which is not suited
for this purpose. The widgets are very few and the framework is not
suited to create complex User Interfaces.&lt;/p&gt;

&lt;h3&gt;The very bad thing to do&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;You should not do this.&lt;/strong&gt; Unfortunately, this is how it works for most.&lt;/p&gt;

&lt;p&gt;A very bad programmer is going to create a web page for each of the
different aspects of its program. The web page would be dynamically
generated on the server using a template engine. Some JavaScript would
be included in this mess to avoid refreshing a page while in fact you
should.&lt;/p&gt;

&lt;p&gt;This is bad because you'll have a ton of page refresh, and that's bad
for the user. If you're not using JavaScript, this is the old way of web
applications and is acceptable, but if you're using JavaScript for more
than a few things, it's bad.&lt;/p&gt;

&lt;p&gt;You shouldn't use JavaScript to change the content of a page to match
what the server would generate just to avoid a refresh. Just because
this is just a way to recreate a template engine on the client that is
redundant with the one on the server.&lt;/p&gt;

&lt;p&gt;And if you are using JavaScript to change the content of a page to get
the content that you should generally see in another page, you're a
moron. This breaks the back/forward mechanism and is very bad.&lt;/p&gt;

&lt;p&gt;The only sensible thing to do  in this configuration with JavaScript is
to script the user interface. Show hidden sections, enable a button.
Don't contact the server using AJAX, you are already getting information
from the server using normal page reload.&lt;/p&gt;

&lt;h3&gt;The sensible thing to do&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Please do this!!!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;An application is an application. It's a bundle and can't be separated
into pages. It can be separated into sub-sections if you want, but those
will necessarily trigger a page refresh.&lt;/p&gt;

&lt;p&gt;The application should be the equal of a desktop application using a
native toolkit, except that web technologies are used instead.
Contacting the server should be limited to what is necessary only
(fetching resources, exchange data, ...). In particular, the templates
should be on the client side.&lt;/p&gt;

&lt;p&gt;On the server side, you'd have just a classic Web API, and static
resources. The Web API should be designed with care and security in
mind. It should be easily shared with third parties that want to
integrate within your application.&lt;/p&gt;

&lt;p&gt;It is as simple as that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server = Web API + Resources&lt;/li&gt;
&lt;li&gt;Client = UI (incl templates)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I heard that SproutCore is a solution to work this way, but it was
horrible to use it as well.&lt;/p&gt;

&lt;p&gt;Perhaps, we need to get away from frameworks, there are many solutions
that can integrate well together and that don't need a framework. Read
this article for example: &lt;a href=&quot;http://blog.getify.com/grab-ui-by-the-handlebar/&quot;&gt;Grab UI by the Handlebar&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;An Example&lt;/h2&gt;

&lt;p&gt;I have a very simple example to show you what I mean, the comment
mechanism on this website. Each page contain a JavaScript script (just
look at the sources) that will add the comment features. The page
delivered by the server contain no information about the comments,
except a &lt;code&gt;&amp;lt;noscript&amp;gt;&lt;/code&gt; tag to tell the people without JavaScript that
they miss the comments.&lt;/p&gt;

&lt;p&gt;The script does an AJAX query on a third party server (with PHP and a
database). The arguments are the URL of the page and the answer is the
JSON formatted list of comments for the page. These comments are then
presented on the page. (The refresh link does this again)&lt;/p&gt;

&lt;p&gt;The script also creates a form to add a comment. When the form is
submitted, an AJAX query is made with the URL of the page and the
content of the comment as arguments.&lt;/p&gt;

&lt;p&gt;This is how all web applications should work.&lt;/p&gt;

&lt;h2&gt;Going further: static databases on the server&lt;/h2&gt;

&lt;p&gt;I already wrote an article on this, but I'll summarize the idea here.&lt;/p&gt;

&lt;p&gt;With this new idea for web applications, the application logic is moved
from the server to the client. Perhaps not all the way, but for simple
applications like the comments above, the server API is nothing much
than a fancy database API.&lt;/p&gt;

&lt;p&gt;Why not model a database on this model? But it already exists:
&lt;a href=&quot;https://en.wikipedia.org/wiki/CouchDB&quot;&gt;CouchDB&lt;/a&gt;. This is a database which API is a web API. I want to
take the principles of this database and mix them with the ideas of a
static page generator.&lt;/p&gt;

&lt;p&gt;The idea is that read only access when the resource URL is known, a
static page should correspond to the URL and a classic web server should
answer the query.&lt;/p&gt;

&lt;p&gt;Only update and search queries would be filtered by the web server to a
FastCGI application that is going to update the static files, or look at
them to answer the search.&lt;/p&gt;

&lt;p&gt;I find it difficult to find advantages of this compared to CouchDB.
There is one thing, your data will always be accessible read only. No
obscure database format that require a database server that might not
work on new hardware, might not be maintained any more...&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With this approach, most web applications could be composed of static
assets that are accessing a database with a Web API. The static assets
and the database could be on the same server or on a different one. No
limit is posed here, except the same origin restriction of web browsers.&lt;/p&gt;

&lt;p&gt;Fortunately, a new standard (that I'm using for the comments of this
website) let you specify if the same origin policy should apply or not:
&lt;a href=&quot;http://www.w3.org/TR/access-control/&quot;&gt;Cross-Origin Resource Sharing&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;What about Cookies Then?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;They should not exist in their current form.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This was a late addition to the HTTP standard, because it was already
implemented. It was never really thought of, and is a breach of the
original HTTP model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The cookies are what allows XSS vulnerabilities, NOT JavaScript&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current web community takes the things the wrong way. Instead of
damning cookies forever, they blame on JavaScript and try to build walls
all over the place to avoid XSS while there is a most simple solution.&lt;/p&gt;

&lt;h3&gt;What are XSS?&lt;/h3&gt;

&lt;blockquote&gt;&lt;p&gt;Cross-site scripting (XSS) is a type of computer security vulnerability typically found in Web applications, such as web browsers through breaches of browser security, that enables attackers to inject client-side script into Web pages viewed by other users. A cross-site scripting vulnerability may be used by attackers to bypass access controls such as the same origin policy. Cross-site scripting carried out on websites accounted for roughly 84% of all security vulnerabilities documented by Symantec as of 2007.[1] Their effect may range from a petty nuisance to a significant security risk, depending on the sensitivity of the data handled by the vulnerable site and the nature of any security mitigation implemented by the site's owner.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;(Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-site_scripting&quot;&gt;Wikipedia&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;This is a wrong way to look at the problem. Inclusion of other items in
web pages dates from the very beginning. It is the foundation of the
web. What allows to have rich documents, with links between them. The
same origin policy is just one of these walls that I talked about, that
assumes that all pages on the current domain can be trusted. Is that true?&lt;/p&gt;

&lt;p&gt;The real story is that if the design of the web was correct, the script
injected by attackers in web pages would be completely harmless. The
same origin policy shouldn't exist and bypassing it should be by no mean
be a security risk.&lt;/p&gt;

&lt;p&gt;We have come to a situation where WebGL can be a security risk because
it can determine the colours of an image included in the web page. Just
because an image can, from the very beginning of the web, bypass the
same origin policy. Some people say that images should respect the same
origin policy, but what they don't imagine is that we could just forget
all of this mess.&lt;/p&gt;

&lt;h3&gt;The Real Problem.&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;real&lt;/em&gt; problem is that when loading foreign content, though AJAX,
the use of images or whatever, a page can load a foreign page where you
are logged in using cookies. And access your foreign account. For
example your bank account.&lt;/p&gt;

&lt;p&gt;The problem is that the page containing the XSS script was granted the
access to the cookie protecting your bank account.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why the hell your bank account is just protected by a cookie than any
page can have access to???&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;The solution&lt;/h3&gt;

&lt;p&gt;Unless you have in your address bar the address of your bank, the cookie
protecting your bank account should be locked away. In fact, instead of
having everything in a page respect the same origin policy, &lt;strong&gt;cookies
should be the single thing respecting the same origin policy;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is just a change in the handling of the cookie by the browser. Not
a very big change at that. Only cookies for the domain in the address
bar should be exploitable. But it would break a number of assumption:&lt;/p&gt;

&lt;p&gt;Third party cookies would no longer exist, this would be a big shock for
advertisers and tracking programs. A loss for these companies, but a win
for personal privacy.&lt;/p&gt;

&lt;h4&gt;Allow third party cookies nonetheless&lt;/h4&gt;

&lt;p&gt;We could nonetheless allow third party cookies, but only in the context
of a specific page:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;each tab/window of the browser share the same cookie jar for the requests&lt;/li&gt;
&lt;li&gt;for each tab/window, a separate cookie jar would be assigned that will
be emptied when a link is followed to a different origin&lt;/li&gt;
&lt;li&gt;the sub-elements of a page that are not of the same origin of the page
itself, should have their cookies in the sub cookie jar only&lt;/li&gt;
&lt;li&gt;if there are sub-sub elements, they should be assigned to a sub-sub
cookie jar recursively.&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Allow third party cookies to access the main cookie jar on demand&lt;/h4&gt;

&lt;p&gt;This would nonetheless break a number of things. Third party scripts
expect to see the same cookies they set a while ago, even if it was not
on the same page.&lt;/p&gt;

&lt;p&gt;A solution to this problem would be to add an option in the browser UI
to specifically allow a third party script in a page to access the
global cookie jar. There would be a button saying for example &quot;Some
cookies have been blocked&quot;. When clicked it would open a menu with:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Allow access to ad.doubleclick.com&lt;/li&gt;
&lt;li&gt;Allow access to facebook.com&lt;/li&gt;
&lt;li&gt;Allow access to my-bank.com&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;A sensible person would not allow a blog with an XSS script access his
bank website, but would allow facebook if he wants to use a facebook button.&lt;/p&gt;

&lt;h4&gt;Browser plugins, anyone ?&lt;/h4&gt;

&lt;p&gt;I'd love a browser plugin for that !&lt;/p&gt;

&lt;p&gt;Unfortunately, on Firefox, creating sub cookie jar would be difficult
due to the architecture of it. Webkit is better on that.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Cookies aren't necessarily bad, they should just respect the same origin
policy they introduced instead of imposing it to all of the other
elements of a page. This can already be done now in web browsers, it
just needs a consensus. Or at least, an extension with the right UI to
bypass the same origin on demand when the scripts needs to.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">WWWGen : Static Page Generator and Beyond</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/07/26/wwwgen_static_page_generator_and_beyond/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/07/26/wwwgen_static_page_generator_and_beyond/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-07-26T10:57:35+02:00</published>
    
    <content type="html">&lt;p&gt;I want to create a new static webpage generator. There are tons of them,
you'll tell me, why do I want to create a new one?&lt;/p&gt;

&lt;h2&gt;Static Website Generator&lt;/h2&gt;

&lt;p&gt;Let's see what we have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;webby: my first love&lt;/li&gt;
&lt;li&gt;nanoc: the second one I used&lt;/li&gt;
&lt;li&gt;webgen: the one I'm using currently (with tons of local modifications)&lt;/li&gt;
&lt;li&gt;jekyll: powering GitHub pages&lt;/li&gt;
&lt;li&gt;and many more most probably&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Most of these systems take the assumption that you are a developer and
you are comfortable to write code to have your website running. In all
of these systems, I was forced to code some Ruby here and there to get
it working how I wanted to.&lt;/p&gt;

&lt;p&gt;The case of nanoc3 is very special because the main configuration uses a
special Ruby DSL to manage the path locations. If you have non technical
users, they won't be willing to write code in this file anyway. And for
technical users, it might not be powerful enough.&lt;/p&gt;

&lt;p&gt;Jekyll is probably the simplest system and can probably be used by non
technical users, but it is far too simple and not powerful enough.
That's why I didn't used it.&lt;/p&gt;

&lt;p&gt;In the end, I modified Webgen a lot to include the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ability to write ruby code in haml instead on having to rely on the
template engine included in webgen&lt;/li&gt;
&lt;li&gt;special .index nodes that will generate a paginated index of articles
found dynamically. The index would contain the last articles in reverse
order while the pages will contain each N articles in the natural order.
This way, an article that end up on page 3 is always going to be on page 3.&lt;/li&gt;
&lt;li&gt;special .tags nodes that will generate .index pages dynamically to
create an index or articles for each tag.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If you look around, there are not many static web page generators that
permit that. First, I decided I would maintain my own version of webgen
with these modifications, but now, I have the idea that the code base is
so horrible that I prefer rewrite the same functions from scratch.&lt;/p&gt;

&lt;h2&gt;Rewriting From Scratch&lt;/h2&gt;

&lt;p&gt;As I said, I'm not satisfied with the current status of the webgen code.
There is a complex system of cache, and a blackboard object that is used
to dispatch method call to the correct objects around the system. The
problem is that this extra indirection level makes it difficult to know
the code path. It would be useful if the hooks in the blackboard would
be highly dynamic, but it's mostly static. it serves no purpose whatsoever.&lt;/p&gt;

&lt;p&gt;Moreover, I don't like big programs that do everything. And all of these
static website generators have a component that is used to determine
which pages are out of date, and only update them. This is traditionally
what make(1) should do. And recently, I found that
&lt;a href=&quot;https://github.com/apenwarr/redo/&quot;&gt;redo&lt;/a&gt; does the job very well. So, I want it to be an integral
part of my new system.&lt;/p&gt;

&lt;h2&gt;WWWSupport&lt;/h2&gt;

&lt;p&gt;Recently, I wrote a piece of code: &lt;a href=&quot;http://git.mildred.fr/?p=pub/mildred/wwwsupport.git;a=tree&quot;&gt;WWWSupport&lt;/a&gt;. It's a very
simple git repository that is supposed to be included as a submodule of
the git repository of a website. it contains a
&lt;a href=&quot;http://cr.yp.to/daemontools.html&quot;&gt;daemontools&lt;/a&gt; daemon that receives e-mail from a special
mailbox and convert them into blog posts on the fly (that's how I'm
currently writing this block post).&lt;/p&gt;

&lt;p&gt;I want my WWWGen project to integrate the same way into my website.&lt;/p&gt;

&lt;h2&gt;WWWGen&lt;/h2&gt;

&lt;p&gt;WWWGen is the name of my website generator. The architecture is very simple:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;src&lt;/code&gt; directory containing the pages, in the same format as webgen&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;nodes&lt;/code&gt; directory containing the files WWWGen is working on to
generate output&lt;/li&gt;
&lt;li&gt;An output directory where the result files in &lt;code&gt;nodes&lt;/code&gt; is copied to,
and where some static assets are copied as well (using rsync)&lt;/li&gt;
&lt;li&gt;A redo script &lt;code&gt;all.do&lt;/code&gt; that contains the configuration and invokes the
wwwgen script.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The wwwgen script will create the &lt;code&gt;nodes&lt;/code&gt; directory and the redo scripts
necessary to its working in it. Then, it will do three things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For each source file, the script will create:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A .node file that contain a relative path to the source file, and
represents it.&lt;/li&gt;
&lt;li&gt;As many .outdep files as the source file will generate output
files. The .outdep file is not managed by redo (because redo doesn't
support multiple targets yet). It references the .node file using a
relative path.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  Note that during this process, new sources can be generated to allow
to create new nodes. This step will be executed until no new sources are
generated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once this is done, the main script will look for all of the .outdep
files and will build the corresponding .out file. The .out file will
contain the final processed result of the page&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy all .out files in the output directory (after removing the .out
extension) and all the static files in the static directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Note that step 1 and 2 recursively call redo to generate the .node and
.out files. This two step design is necessary to account for multiple
pages generated from a single source.&lt;/p&gt;

&lt;h2&gt;Beyond&lt;/h2&gt;

&lt;p&gt;In all my projects, I always want to focus on the usability of what I
create. I always think that non programmers should be able to do the
same that I did, to a certain limit. For example, my personal e-mail
server at home is scripted all the way. Reinstalling it should be a
matter of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;installing a debian system with a basic configuration&lt;/li&gt;
&lt;li&gt;clone my git repositories&lt;/li&gt;
&lt;li&gt;Set some basic configuration (hostname, ...)&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;redo install&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I agree that even then, not anybody can install at home a mail server,
but with a process that simple, it's possible to create user interfaces
for it. So even if it's not there, it's a possibility.&lt;/p&gt;

&lt;p&gt;I want the same for WWWGen. It leaves a possibility for creating a user
interface. Nothing prevents from creating a web application or even a
native application, that will create markdown pages with a WYSIWYG
editor (‡ la WordPress). The page files thus created could be checked
out in a git repository and pushed to a server. There, a hook will run
WWWGen to update the website with the modifications.&lt;/p&gt;

&lt;p&gt;This could be seriously a very good alternative to WordPress, and I'd
preefer working with such a system than WordPress.&lt;/p&gt;

&lt;h2&gt;What already exists&lt;/h2&gt;

&lt;p&gt;I am not very good at creating desktop applications. So I thought I
would reuse the existing ones : my mailer. I's like a Content Management
System where everything must be configured by hand, and only articles
can be submitted using an e-mail.&lt;/p&gt;

&lt;p&gt;This post is being sent by e-mail to my personal web server. Currently,
I'm still using plain text with a markdown syntax, but we could reuse
the HTML markup in a mail. This e-mail is then processed by a special
alias in &lt;code&gt;/etc/aliases&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; http.www: &quot;|/srv/http/www/wwwsupport/push.sh&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line was automatically generated by an install.do script. The
push.sh script will read the mail using a ruby script and will create a
file in my website git repository with the content of the mail. Then
webgen is run and the content of the repository is pushed to origin.&lt;/p&gt;

&lt;p&gt;As a consequence, the new article appears on the website. This is a very
simple form of user interface, but it permits anybody to write blog posts.&lt;/p&gt;

&lt;h2&gt;What features I would like to see&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Better parsing of HTML e-mail&lt;/li&gt;
&lt;li&gt;Using wwwgen instead of webgen&lt;/li&gt;
&lt;li&gt;Support for image galleries using git-annex in wwwgen&lt;/li&gt;
&lt;li&gt;Support for taking the attached images in an e-mail to create a
gallery on my website?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;What features a customer would want&lt;/h2&gt;

&lt;p&gt;A web application that can :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;modify the website configuration in the &lt;code&gt;all.do&lt;/code&gt; file&lt;/li&gt;
&lt;li&gt;modify any source files, using a WYSIWYG editor when applicable&lt;/li&gt;
&lt;li&gt;add static assets (possibly using git-annex)&lt;/li&gt;
&lt;li&gt;run the website compilation and preview it&lt;/li&gt;
&lt;li&gt;push to a server to update the production website&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is entirely possible.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Static database over HTTP</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/07/06/static_database_over_http/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/07/06/static_database_over_http/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-07-06T11:08:36+02:00</published>
    
    <content type="html">&lt;p&gt;Websites should be static. Any dynamic thingy (scripting using php,
ruby, python and the like) should be limited to a set of features that
absolutely cannot be implemented otherwise. Most GET requests should
lead to a static resolution (there can be exceptions such as GET queries
of search engines for instance).&lt;/p&gt;

&lt;p&gt;Respecting this principle is quite simple, just generate the static
pages when they are updated, and no not worry about them afterwards.
This is what I try to use for my website, and it works quite well.&lt;/p&gt;

&lt;p&gt;Advantages of this technique :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;your data will always be readable in the future, even if you may not
write it any more&lt;/li&gt;
&lt;li&gt;improved security: normal operations only involve static files. You
get to spend more time with update actions (POST and PUT) and design
them better.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now, I had the idea to extend this to databases. Do you know about
&lt;a href=&quot;https://en.wikipedia.org/wiki/CouchDB&quot;&gt;CouchDB&lt;/a&gt; ? It's a database which
has a web interface only. I very like its design but again, I'd like it
to use the same principle as above.&lt;/p&gt;

&lt;p&gt;The idea of such a database came with the feature I developed for my
blog: the user comments. In this blog, the user comments are completely
managed with JavaScript. If you don't have JavaScript, you don't have
comments at all. How do that work ?&lt;/p&gt;

&lt;h2&gt;Comments&lt;/h2&gt;

&lt;p&gt;To get the comments for an article, the JavaScript will contact a simple
PHP application in another server (a free hosting service). This simple
application is able to store and get JSON data using REST requests. The
JavaScript will then use &lt;code&gt;XmlHttpRequest&lt;/code&gt; to contact the server and give
it the canonical URL (&lt;code&gt;&amp;lt;link rel=canonical&amp;gt;&lt;/code&gt;). The server will answer a
JSON object with the comments.&lt;/p&gt;

&lt;p&gt;Storing a comments is done the same way, using a &lt;code&gt;POST&lt;/code&gt; request instead
of a &lt;code&gt;GET&lt;/code&gt; request.&lt;/p&gt;

&lt;h2&gt;To a Database Server&lt;/h2&gt;

&lt;p&gt;This is very simple yet powerful. Why not extend this design to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;allow any kind of data, not just comments&lt;/li&gt;
&lt;li&gt;allow simple &lt;code&gt;GET&lt;/code&gt; requests to bypass any script and just fetch the
raw data&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We can imagine the data store to be publicly accessible using URL that
end up with the &lt;code&gt;.json&lt;/code&gt; suffix. There would be a similar URL with
&lt;code&gt;.json.meta&lt;/code&gt; to access the metadata about an object (its current
version, right access, ...). We can imagine the web applications of the
future being completely implemented on the client side. The server side
would be just a shared database.&lt;/p&gt;

&lt;p&gt;We would obviously need a security layer to prevent anyone to read
anything if they should not be allowed. We can imagine three levels of
permissions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;read and write by everyone&lt;/li&gt;
&lt;li&gt;read by everyone, write only by authorized user&lt;/li&gt;
&lt;li&gt;read and write only by authorized user&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We could imagine many different authentication mechanisms. For most
data, the mechanism could be of a shared secret. The metadata of a json
file would contain :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; &quot;auth&quot;: &quot;shared-secret&quot;,
 &quot;secret&quot;: &quot;path/to/another/file&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get access to the file, the client would have to provide the exact
content of the file &lt;code&gt;&quot;path/to/another/file&quot;&lt;/code&gt;, which would obviously be a
protected file, readable only by authorized access. It could be a
login/password or anything else.&lt;/p&gt;

&lt;p&gt;Update operations would be :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PUT&lt;/code&gt;: to update the entire content of the file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;POST&lt;/code&gt;: append to the existing data (the data should be a JSON array)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The data file will have an associated version which will be in the form
of &quot;sha1:&lt;sha1 of the file&gt;&quot;. To successfully update a data file, the
existing version of the file must be given. If it is not the same, the
client should retry. This is the same concept as in CouchDB.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">A new way to update my blog</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/06/21/a_new_way_to_update_my_blog/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/06/21/a_new_way_to_update_my_blog/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-06-21T15:08:09+02:00</published>
    
    <content type="html">&lt;p&gt;Hi everyone, I just developed a new way to update my blog. I am just
using my mailer (which is currently Thunderbird for a lack of a better
alternative). As you may know, I already manage my own server. I have a
nginx web server and my website is just plain old static pages generated
from markdown using my own version of webgen.&lt;/p&gt;

&lt;p&gt;My code is available at two locations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://git.mildred.fr&quot;&gt;my personal git server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://github.com/mildred/&quot;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I just developed a few scripts which does the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hooked in /etc/aliases, receive e-mail from a special e-mail address.
Put this mail in an inbox directory on the server&lt;/li&gt;
&lt;li&gt;another daemon (a shell script run by daemontools actually) run as the
www-data user is monitoring the inbox using inotifywait. When a mail
comes, or every 60 seconds, it process the mail using a ruby script that
convert the mail to a markdown page, and run webgen on the website. It
also commit and push the changes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And that's all.&lt;/p&gt;

&lt;p&gt;The one thing I couldn't automate is the creation of the private key to
push the changes back to the git server.&lt;/p&gt;

&lt;p&gt;All the code &lt;a href=&quot;http://git.mildred.fr/?p=pub/mildred/wwwsupport.git;a=summary&quot;&gt;is
there&lt;/a&gt;. I
just don't know if I have a public URl for these repos :)&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Migration to Webgen</title>
    
    <author>
      <name>Mildred Ki'Lya</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2012/02/28/migration_to_webgen/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2012/02/28/migration_to_webgen/index.html</id>
    <updated>2012-10-19T21:03:17+02:00</updated>
    
    <published>2012-02-28T11:07:07+01:00</published>
    
    <content type="html">&lt;p&gt;Webgen allows me to split a post in two, an excerpt and a content. Perhaps the
name excerpt is ill chosen as currently it seems more like a teaser. Anyway.&lt;/p&gt;

&lt;p&gt;I already improved or added the following features in webgen:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;an index content handler type which job is to list other pages' content and
provide pagination. It also create atom and rss feeds.&lt;/li&gt;
&lt;li&gt;a tag content handler that creates index pages, thus creating pagination and
feeds for each and every tag.&lt;/li&gt;
&lt;li&gt;ability to include other pages' content directly from within haml or erb.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Next, I'll want to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Have a cron job that updates the website automatically, or a git hook doing
just that.&lt;/li&gt;
&lt;li&gt;Look at what I can do in terms of a gallery.&lt;/li&gt;
&lt;li&gt;Parse e-mails sent to a special mailbox to create blog post from them
automatically&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;All of this is released open source.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Freedombox</title>
    
    <link href="http://mildred.fr/Blog/2011/03/22/freedombox/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2011/03/22/freedombox/index.html</id>
    <updated>2012-04-10T16:57:20+02:00</updated>
    
    <published>2011-03-22T09:26:44+01:00</published>
    
    <content type="html">&lt;p&gt;Yesterday, I realized that we all needed to protect our privacy and that we
might be facing something more than just ads. The solution, fortunately it was started: &lt;a href=&quot;http://www.freedomboxfoundation.org/&quot;&gt;Freedombox&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is a specific part I want to cover: protected communication channels.
Starting with E-Mails.&lt;/p&gt;

&lt;p&gt;In the E-Mail world, GMail is the best, in my opinion, except that it is hoisted
on Google servers. How about having a private GMail on your freedombox ? What
are the use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We need to know which server sent the E-mail or tell the user if we can't
know. Solutions exists like &lt;a href=&quot;http://en.wikipedia.org/wiki/DomainKeys_Identified_Mail&quot;&gt;DKIM&lt;/a&gt;,
&lt;a href=&quot;http://en.wikipedia.org/wiki/DomainKeys&quot;&gt;DomainKeys&lt;/a&gt; or &lt;a href=&quot;http://en.wikipedia.org/wiki/Sender_Policy_Framework&quot;&gt;SPF&lt;/a&gt;.
The solution liew with all of thesesolution, not merely just one.&lt;/p&gt;

&lt;p&gt;What we should see is before the E-mail a little line telling something
like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We could not determine which server sent the E-Mail, it might be spam or
scam.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We could not determine which server sent the E-Mail, it appears to come
from google.com but google.com generally signs outgoing E-mails. This is
probably spam or scam.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The E-Mail was sent from google.com&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The E-Mail was sent from toto31.freedombox.net and has been signed by
sender@toto31.freedombox.net.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The E-Mail was sent from toto31.freedombox.net and has been encrypted by
sender@toto31.freedombox.net.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now, the freedombox provides many services. For example we need to have a PGP
Key server. How do we advertise that? DNS was made for that. I was thinking
specifically about &lt;a href=&quot;http://avahi.org/&quot;&gt;Avahi&lt;/a&gt;, providing Multicast-DNS on the
local network. I think we need to either transform Avahi into a full DNS server
that could run on the Freedombox or have it publish records in an existing DNS
server on the box. Why? Because services are already used to publish DNS records
using Avahi.&lt;/p&gt;

&lt;p&gt;We also need to have a network of Freedombox to add redundancy to our DNS
servers. That could be implemented as a second part.
It would also be good if we could have a domain like freedombox.net where all
freedombox could have a free subdomain for free. Domain names are a necessity.&lt;/p&gt;

&lt;p&gt;Now, what I want to do is simple:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Buy a plug computer and install Freedombox on it&lt;/li&gt;
&lt;li&gt;Work on integrating a SMTP server that would

&lt;ul&gt;
&lt;li&gt;send signed E-Mails&lt;/li&gt;
&lt;li&gt;receive E-Mails and filter them&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Integrate an IMAP server&lt;/li&gt;
&lt;li&gt;Integrate a PGP Key server&lt;/li&gt;
&lt;li&gt;Work on integration with DNS&lt;/li&gt;
&lt;li&gt;Work on a client that would sign e-mails, send them and open the inbox on
IMAP&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now, it would be freat to have in all modern browsers a &lt;code&gt;dns:&lt;/code&gt; scheme which
would present all the services of a specific server. For example it could tell
you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This server provides a web server. &lt;em&gt;Browse&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;This server provides a LDAP address book. &lt;em&gt;Browse&lt;/em&gt; &lt;em&gt;Search&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;This server provides an XMPP server. &lt;em&gt;Sign-In&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;This server provides a SMTP server. &lt;em&gt;Send E-Mail&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This would be awesome.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title type="html">Bug 660340 - Due to constant threats on privacy, epiphany should allow the user to have multiple separate session running</title>
    
    <author>
      <name>Mildred</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2011/09/28/bug_660340_-_due_to_constant_threats_on_privacy_epiphany_should_allow_the_user_to_have_multiple_separate_session_running/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2011/09/28/bug_660340_-_due_to_constant_threats_on_privacy_epiphany_should_allow_the_user_to_have_multiple_separate_session_running/index.html</id>
    <updated>2012-04-10T16:57:20+02:00</updated>
    
    <published>2011-09-28T10:42:04+02:00</published>
    
    <content type="html">&lt;p&gt;Here is my &lt;a href=&quot;https://bugzilla.gnome.org/show_bug.cgi?id=660340&quot;&gt;Bug 660340&lt;/a&gt;. I
created it after looking at the recent facebook 'enhancements' that makes
privacy even more precious (&lt;a href=&quot;http://linuxfr.org/news/facebook-f8-timeline-musiquevid%C3%A9o-ticker-boutons-et-les-cons%C3%A9quences-pour-le-web&quot;&gt;article in French&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;We need to quickly find a way to preserve our privacy on the Internet.&lt;/p&gt;

&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;With the recent threats of the various big internet companies on our privacy,
it would be a great enhancement if epiphany allowed to have separate navigation
contexts (cookies, HTML5 storage, cache) at will, and easily.&lt;/p&gt;

&lt;p&gt;Some companies, especially facebook, and I suppose Google could do that as
well, can use all kind of methods to track a user usiquely. using cache, HTML5
storage or cookies. I wonder if they can use the cache as well, but I heard it
could be prevented. Firefox does.&lt;/p&gt;

&lt;p&gt;One solution to counter these privacy threats is to have a different browser,
or different browser profile, for each of the web sites we load. This is
however very inconvenient, and it should made easily possible.&lt;/p&gt;

&lt;p&gt;First let define the concept of session. A session is almost like a separate
instance of the browser. It share bookmark and preferences with other session,
but have separate cache, separate set of cookies and separate HTML5 DOM
storage.&lt;/p&gt;

&lt;p&gt;I imagine the following behaviour, based on the document.domain of the toplevel
document:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If a page is loaded without referrer, and the domain is not associated with
an existing session, start a new session for that domain&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a page is loaded without referrer, and the domain is a domain that is
already associated with an existing session, then prompt non intrusively:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;You already opened a session on example.com.&quot;
Choices: [  Start a new session    ‚ñº ] [Use this session]
         [[ New anonymous session   ]]
         [  Replace existing session ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the session is started anonymously, it would not be considered for reuse&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a page is loaded using a link from an existing window/tab, and the
domain
is the same, then share the session&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a page is loaded using a link from an existing window/tab, and the
domain
is NOT the same and, then a non intrusive message is displayed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;You are now visiting example2.com. Do you want to continue your session
from example1.com?&quot;
Choices: [  Start a new session    ‚ñº ] [Share previous session]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &quot;Start a new session&quot; dropdown menu changes if the example2.com is
already associated with a session or not. If example2.com is associated
with a session:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; [[ New anonymous session            ]]
 [  Replace example2.com session      ]
 [  Use existing example2.com session ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If example2.com is not associated with a session:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; [[ New anonymous session    ]]
 [  New example2.com session  ]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The choices in [[xxx]] (as opposed to [xxx]) is the most privacy enhancing one,
and would be the default if the user choose in the preferences&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[x] allow me not to be tracked
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The messages are non intrusive, they can be displayed as a banner on top of the
page. The page is first loaded with the default choice, and if the user decides
to use the other choice, the page will be reloaded accordingly (or the session
will be reassigned).&lt;/p&gt;

&lt;p&gt;This setting can traditionally be used to set the do not track header&lt;/p&gt;

&lt;p&gt;Every settings should have a setting &quot;do not prompt me again&quot; that could be
reset at some point.&lt;/p&gt;

&lt;p&gt;About embedded content: Because toplevel pages do not share the same session
(toplevel page opened at example.com have a different session than toplevel
page opened at blowmyprivacy.org), if a page from example.com have embedded
content from blowmyprivacy.org, the embedded content would not be able to track
the user, except within the example.com website.&lt;/p&gt;

&lt;p&gt;It is possible to imagine global settings that would hide some complexity:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ ] When I load a page, always associate it with its existing session.
[ ] When I switch website, always reuse the existing session of the new
    website.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This user interface might seem complex at first, but it is far less complex
than letting the user deal with different browser profiles by hand.
Unfortunately, I don't think we can abstract privacy that easily. Keep in mind
that all of these settings would be enabled only if the user choose to enable
privacy settings.&lt;/p&gt;

&lt;p&gt;I am ready to contribute to the implementation of this highly important feature
(important for our future and our privacy). Do you think an extension might be
able to do all of that, or do you think the browser code should be modified?&lt;/p&gt;

&lt;p&gt;Further possible enhancements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Add another choice for anonymous sessions using Tor (or I2P)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add the possibility to have multiple session registered with the same
domain. This would enable the user to have different profiles for the same
website.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  
  <entry>
    <title type="html">HTML 2.0: Why I think HTML is broken</title>
    
    <author>
      <name>Shanti</name>
      <uri></uri>
    </author>
    
    <link href="http://mildred.fr/Blog/2011/08/05/html_2_0/index.html" rel="alternate" />
    <id>http://mildred.fr/Blog/2011/08/05/html_2_0/index.html</id>
    <updated>2012-04-10T16:57:20+02:00</updated>
    
    <published>2011-08-05T10:35:08+02:00</published>
    
    <content type="html">&lt;p&gt;First postulate: HTML was designed as a stateless protocol&lt;/p&gt;

&lt;p&gt;Context: web sites need to maintain a context (or state) to track the client.
This is required by the log-in procedures the various websites have. It is also
useful to track the user in a web store, to know which items the user wants to
buy. In fact, it is requires almost everywhere.&lt;/p&gt;

&lt;p&gt;The first solution to be thrown out for this problem are the cookies. People
didn't like cookies but now, everyone accepts them. Nothing works without
cookies. Why did people dislike cookies back then? They liked their provacy and
cookies makes it possible to track the user. Through advertisement networks, the
advertiser known exactly which website the user visited. And it is still the
case now. What changed is that the users got tried to fight cookies and have
every website break, and they got used to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;People got used to being tracked&lt;/strong&gt; just as people are used to be watched by
video cameras in the street and people are used to get tracked by the government
and big companies and banks.&lt;/p&gt;

&lt;p&gt;Cookies are a great way to track prople, all because HTTP didn't include session
management. The way Google track you is very simple. Google Analytics puts a
cookie on your computer and each time you access the Google Server, they know
it's the same person. Google is everywhere:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Many web sites are using Google APIs, or the jQuery library at Google.&lt;/li&gt;
&lt;li&gt;Many web sites ask Google to track their users to know how many prople visit
their page.&lt;/li&gt;
&lt;li&gt;Google makes advertisement.&lt;/li&gt;
&lt;li&gt;Youtube, Blogger, Picasa and others are owned by Google&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With this alone, Google is found on almost every page. If you have an account at
Google (YouTube, Picase, Gmail, Blogger, Android or other), they can even give a
name or an e-mail address to all of these information.&lt;/p&gt;

&lt;p&gt;Google motto is &lt;em&gt;Don't be Evil&lt;/em&gt;, they are perhaps not evil but can they become
evil? Yes.&lt;/p&gt;

&lt;p&gt;Whatever, my dream HTTP 2.0 protocol would include of course push support like
WebSockets, but more importantly: session management. How should this be done?&lt;/p&gt;

&lt;h3&gt;HTTP and Session Management&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;When the server needs a session, it initiates the session by giving a session
token to the client.&lt;/strong&gt; The client needs to protect this token from being stolen
and should display that a session is in pogress for this website. It could
appear on the URL bar for example. The client could close the session at any
moment.&lt;/p&gt;

&lt;p&gt;With the token, the server provides its validity scope. Domains, subdomains,
path. Only the resources in the session scope will receive the tocken back. If
for example &lt;code&gt;http://example.com&lt;/code&gt; starts a session at &lt;code&gt;example.com&lt;/code&gt; but have an
&lt;code&gt;&amp;lt;iframe&amp;gt;&lt;/code&gt; that includes facebook. Facebook won't receive the session token. If
Facebook wants to start a session (because the user wants to log-in) it will
start a second session.&lt;/p&gt;

&lt;p&gt;Session cannot escape the page. If you have two tabs open with facebook in each
tab (either full page or embedded), the two facebook instances don't share the
same session, unless the user explicitely allowed this. For instance, when
Facebook starts a session, the browser could tell the user that Facebook already
have an existing session and the user would be free to choose between the new
session and the existing one.&lt;/p&gt;

&lt;h3&gt;How does this solve XSS&lt;/h3&gt;

&lt;p&gt;XSS is when a website you don't trust access the session of a website you trust,
and steal it. At least I think so.&lt;/p&gt;

&lt;p&gt;With this kind of session management, the session couldn't possibly be stolen.
Suppose that the non-trusted site makes an XmlHttpRequest to gmail.com. If
cross-domain wasn't forbidden, any web-site could read your mails.&lt;/p&gt;

&lt;p&gt;With the new session management, if the untrusted site makes a request to
gmail.com, gmail.com session wouldn't be available and the login page would be
returned instead of the list of e-mails. If the non trusted website tries to
log-in, you would be prompted to associate the Gmail session with the site you
don't trust. If you aren't completely idion, you wouldn't allow the online
pharmacy to connect to Gmail.&lt;/p&gt;

&lt;h3&gt;Extra&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;What is known about you?&lt;/strong&gt; Let's take an average person that uses her credit
card, have and Android phone with Gmail, uses Facebook:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All your relationships are known by Google (Gmail, Google+) and Facebook&lt;/li&gt;
&lt;li&gt;All your interests are known by Google and Facebook (Ad Sense track which
website you visit and Facebook have a huge profile on you)&lt;/li&gt;
&lt;li&gt;All your posessions are known to your bank&lt;/li&gt;
&lt;li&gt;Your photograph is known by Google and Facebook (people probably took a
photo of you and placed it on their Android phone synchronized with Google)&lt;/li&gt;
&lt;li&gt;Your location is known (using your Android phone, your credit card, or your
RFID card you use for public transportation)&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If you ever want to keep private, it is becoming very difficult.&lt;/p&gt;
</content>
  </entry>
  
</feed>
